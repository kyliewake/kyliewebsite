---
title: "Project2"
author: "Kylie Wakefield"
date: "4/9/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project 2

  The data for this project is from the United States Census Bureau. The data was taken from a population of individuals over the age of 15 in the year 2018. The variables highlight the percentage of individuals in a specific marital category of the population. The population is defined by five different categories: gender, the age group, income ($), and percentage of individuals in a specific marital status. There are a toatl of eight variables.
  The marital status percentages show those who are married, have never been married, widowed, separated, or divorced. The gender shows whether the percentage reflects information for males or females. There are 6 age groups in which the data was taken: 15-19, 20-34, 35-44, 45-54, 55-64, and 65 or older. The "income" is the average income for each age group (it is the same for males and females). There are 12 observations for each variable, for a total of 96 observations. 

## Setup

```{r}
library(tinytex)
library(readxl)
marriage<-read_excel("/Users/kyliewakefield/Documents/UT/Spring 2020/Website/content/dataproject2.xlsx")
```

## MANOVA/ANOVA Testing

```{r}
#assumptions
library(ggplot2)
library(tidyverse)
ggplot(marriage, aes(x = widowed.percent, y = married.percent)) + geom_point(alpha = .5) + geom_density_2d(h=2) + coord_fixed() + facet_wrap(~age.group)
covmats<-marriage%>%group_by(age.group)%>%do(covs=cov(.[6:7]))
for (i in 2:7) {print(as.character(covmats$age.group[i])); print(covmats$covs[i])}
#manova
manova<-manova(cbind(married.percent,widowed.percent)~age.group, data=marriage)
summary(manova)
#univariate anova
summary.aov(manova)
#post-hoc t-tests for married individuals and age group
pairwise.t.test(marriage$married.percent,marriage$age.group,p.adj="none")
#probability of a type I error
1-(.95^(18))
#Bonferroni correction
0.05/18
```
###Running a manova, anova, and post-hoc t-test on the relationship between age group and percentage for those who were either married or widowed gave a total of 18 tests (1 manova, 2 anovas, and 15 t-tests). The probability of a type I error occurring was 60.28%. To adjust for the potential error, a Bonferroni correction was completed in order to ensure that the overall Type I error rate was kept at 0.05. The correction changed the alpha to 0.0028. The manova test showed that the categories of age group differed significantly for at least one of the dependent variables (p<0.001). The univariate anova tests showed that those who were married.percent was significant between at least one age group category (p<0.0001). The percentage of those who were widowed did not differ significantly between any age group. Using the Bonferroni correction, the post-hoc t-tests were analyzed for married.percent and age.group to see which age group differed in those who were married. (p<0.0015). For the those who were married, age groups 15-19 and 35-44, 45-54 and 15-19, 55-64 and 15-19, and 65 and older and 15-19 were signficant after adjustment(p<0.0021). It is unlikely that all of the assumptions were met for the manova test. Looking at the density plot, there was not a lot of data in order to assess multivariate normality. Looking at the covariance matrices, there was not total homeogeneity. But it is hard to meet all of these assumptions!

## Randomization Testing

```{r}
#creating the data frame
marriedmale<-c(0.9,26.1,59.8,64.1,65.7,69.3)
marriedfemale<-c(1.2,32.9,61.3,62.3,60.0,45.0)

#creating the data.frame
cric<-data.frame(gender=c(rep("marriedmale",6),rep("marriedfemale",6)), percentmarried=c(marriedmale,marriedfemale))
#the null statistics
ggplot(cric,aes(percentmarried,fill=gender))+geom_histogram(bins=6.5)+facet_wrap(~gender,ncol=2)+theme(legend.position="none")
#mean difference
cric%>%group_by(gender)%>%summarize(means=mean(percentmarried))%>%summarize(`mean_diff:`=diff(means))
#randomization
rand_dist<-vector()
for(i in 1:5000){
new<-data.frame(percentmarried=sample(cric$percentmarried),gender=cric$gender) 
rand_dist[i]<-mean(new[new$gender=="marriedmale",]$percentmarried)-
              mean(new[new$gender=="marriedfemale",]$percentmarried)}
#histogram of the data
{hist(rand_dist,main="",ylab=""); abline(v = 3.866667,col="red")}
#p-value
mean(rand_dist>3.866667 | rand_dist< -3.866667)
```
###My randomization tests were to test the relationship between the percentage of the population in 2018 that are married over the age of 15 to gender. Are there more males or females that are married above the age of 15? If the null hypothesis were true, there would be no difference between gender and the percent married. On the contrary, if the null hypothesis were rejected due to a significant p-value, then the percentage of males and females who marry over the age of 15 would be significantly different. Since the p-value is greater than 0.05, the null hypothesis cannot be rejected (0.79>0.05). Therefore, there is not difference between the percentage of males and females who are married over the age of 15. This makes sense since the U.S. population is about 1:1 males:females and since males and females marry equally. 

## Linear Regression Model 

```{r}
library(lmtest)
library(sandwich)
#mean center numeric variable
centered<-data.frame(nevermarried.percent=marriage$nevermarried.percent,married_c=marriage$married.percent-mean(marriage$married.percent),income_c=marriage$income.dollars-mean(marriage$income.dollars))
#regression
fit<-lm(nevermarried.percent ~ income_c*married_c, data = centered)
summary(fit)
#GGplot
new1<-centered
new1$income_c<-mean(centered$income_c)
new1$mean<-predict(fit,new1)
new1$income_c<-mean(marriage$income_c)+sd(marriage$income_c)
new1$plus.sd<-predict(fit,new1)
new1$income_c<-mean(marriage$income_c)-sd(marriage$income_c)
new1$minus.sd<-predict(fit,new1)
newint<-new1%>%select(nevermarried.percent,married_c,mean,plus.sd,minus.sd)%>%gather(income.dollars,value,-nevermarried.percent,-married_c)

mycols<-c("#619CFF","#F8766D","#00BA38")
names(mycols)<-c("-1 sd","mean","+1 sd")
mycols=as.factor(mycols)

ggplot(centered,aes(married_c,nevermarried.percent),group=mycols)+geom_point()+geom_line(data=new1,aes(y=mean,color="mean"))+geom_line(data=new1,aes(y=plus.sd,color="+1 sd"))+geom_line(data=new1,aes(y=minus.sd,color="-1 sd"))+scale_color_manual(values=mycols)+labs(color="income")+theme(legend.position=c(.9,.2))
#assumptions
#assumptions: linearity, normality, homoskedasticity
resids<-fit$residuals
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept = 0, color="red")
ks.test(resids, "pnorm", mean=0, sd(resids))
shapiro.test(resids)
bptest(fit)
#robust SE
coeftest(fit, vcov=vcovHC(fit))
```
###For my linear regression model, I wanted to test how the percentages of individuals who have never been married are affected by income and the percentage of those who are married. When interpreting the regression results, there are multiple coefficients. When income and percent marrried decrease on average, the percent never married increases on average by 3.416e+01.For every increase in income by 1, the average percent never married decreases by -1.017e-03. For every increase in average percent married by 1, the average percent never married decreases by -7.981e-01. There is no significance between income and married percent with the percent never married. The best fitting line: %nevermarried= 3.416e+01 + -1.017e-03(income_c) + -7.981e-01(married_c). The ggplot highlights these relationships. The Kolmogorov-Smirnov test shows that the data is normal (p>0.001). Shapiro-Wilk normality test also highlights that the data is also normal since the p value is greater than 0.001. The ggplot shows that the data is relatively normal. But there are few data points so it is harder to tell. To formally test homoskedasticity, a Breusch-Pagan test was run. Since the p value is greater than 0.001, the data is homoskedastic. After running the robust SE test, the only change was the intercept p value. In the SE test, the p value changed from 0.000606 to  0.02743, so it made the intercept less significant. But the SE are larger for some of the variables in the SE test, so it is best to go with these estimates. 

## Bootstrapping

```{r}
samp_distn<-replicate(5000, {
  boot_dat<-boot_dat<-centered[sample(nrow(centered), replace=TRUE),]
  fit2<-lm(nevermarried.percent~income_c*married_c, data = boot_dat)
  coef(fit2)
})
samp_distn%>%t%>%as.data.frame%>%summarise_all(sd)
```
###The bootstrapped SE's changed significantly for this test. The intercept SE increased from 1.2693e+01 in the robust SE test and from 6.264 in the regression to 21.01. The average income SE increased from 0.001372 in the regression test and decreased from 0.0039165 in the robust SE test to 0.0034. The average married percent SE of 1.8452 in the robust SE and 0.5999 in the linear regression changed to 1.456. The interaction between the average percent married and average income SE changed from 3.2931e-05 in the robust SE test and 2.108e-05 in the linear regression test to 9.431714e-05, which decreased a lot. 

## Logistic Regression

```{r}
class_diag <- function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  ord<-order(probs, decreasing=TRUE)
  probs<-probs[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  data.frame(acc,sens,spec,ppv,auc)
}
#binary categorical variable
marriage$highincome<-if_else(marriage$income.dollars>median(marriage$income.dollars),1,0) 
fit<-glm(highincome~nevermarried.percent+married.percent, data = marriage, family = "binomial"(link = "logit"))
coeftest(fit)

#e^(estimate)
exp(coef(fit))

probs<-predict(fit, type = "response")
class_diag(probs,marriage$highincome)
pred<-ifelse(probs>.5,1,0)

#confusion matrix
table(prediction=pred, truth=marriage$highincome)%>%addmargins

#density plot
density<-marriage
density$highincome<-as.factor(density$highincome)
density$logit<-predict(fit, type = "link")
density%>%ggplot(aes(logit,color=highincome,fill=highincome))+geom_density(alpha=.4)+theme(legend.position = c(.65,.65))+geom_vline(xintercept = 0)+xlab("predictor (logit")

#ROC Curve
marriage$prob<-predict(fit, type = "response")
sens<-function(p, data=marriage, y=highincome) mean(marriage[marriage$highincome==1,]$prob>p)
sensitivity<-sapply(seq(0,1,.01),sens,marriage)
spec<-function(p, data=marriage, y=highincome) mean(marriage[marriage$highincome==0,]$prob<p)
specificity<-sapply(seq(0,1,.01),spec,marriage)
ROC1<-data.frame(sensitivity,specificity,cutoff=seq(0,1,.01))

ROC1$TPR<-sensitivity
ROC1$FPR<-1-specificity

ROC1%>%ggplot(aes(FPR,TPR))+geom_path(size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1))+scale_x_continuous(limits = c(0,1))
#AUC
ROC1<-ROC1[order(-ROC1$cutoff),]
widths<-diff(ROC1$FPR)
heights<-vector()
for(i in 1:100) heights[i]<-ROC1$TPR[i]+ROC1$TPR[i+1]
AUC<-sum(heights*widths/2)
AUC%>%round(3)

#k-fold
set.seed(1234)
k=5
data<-marriage[sample(nrow(marriage)),]
folds<-cut(seq(1:nrow(marriage)),breaks=k,labels=F)
diags<-NULL
for(i in k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  truth<-test$highincome
  fit<-glm(highincome~married.percent+nevermarried.percent, data=train,family = "binomial")
  prob<-predict(fit,newdata = test, type = "response")
  diags<-rbind(diags,class_diag(prob, truth))
}
summarize_all(diags,mean)

``` 
###Logistic regressions predict categorical variables. The highest income is predicted by married.percent and nevermarried.percent. The coefficient estimates show that controlling for percent never married, those with the highest income have 0.215 increase in odds of being married. Controlling for percent married, those with the highest income have 0.065 increase in odds of never being married. The coefficients also show the change in odds through e^(estimate), then these values can help interpret the logs odds and probabilities of the gender when added to the intercept. To test for accuracy, a confusion matrix was run. The accuracy of the logistic regression was 91.67%. The sensitivity or true positive rate (TPR) is 1, which is the probability of detecting if the individual has a high income. The specificity or true negative rate (TNR) is 0.833, which is the probability of testing that the individual has a low income. The precision (PPV) is the proportion of individuals who are have a high income. The PPV is 85.71%. The density plot shows the proportion of false negatives (FN) and false positives (FP). The false positives are shown in the gray area to the right of 0, where we predicted individuals to have a high income. The false negatives are shown in the gray area to the left of zero that we predicted to have a low income. A perfect prediction would have no overlaps, but perfectly centered left and right of 0. A perfect prediction for the ROC curve is when TPR=1, FPR=0, and AUC=1.The ROC curve shows that TPR and FPR have more randomized changes. This could be true by looking at the density plot, where the TP in the blue area is mostly above zero. This data is further shown by calculating the AUC in the ROC plot. The AUC is 0.833. This means that 83.3% of the time those who have a high income will have higher scores than those with a low income. This is a pretty good predictor. The k-fold test showed that the AUC is 1, which is a great predictor.  

## LASSO Regression

```{r}
## lasso with linear regression (and numeric predictors)
library(glmnet)
y<-as.matrix(marriage$income.dollars)
x<-model.matrix(income.dollars~.,data = marriage)[,-1]
x<-scale(x)
cv<-cv.glmnet(x,y) 
lasso<-glmnet(x,y,lambda=cv$lambda.1se)
coef(lasso)

#MSE
fitmse<-lm(income.dollars~.,data = marriage)
yhat<-predict(fitmse)
mean((marriage$income.dollars-yhat)^2)
#cross validating from linear regression
set.seed(1234)
k=5 
data<-marriage[sample(nrow(marriage)),] 
folds<-cut(seq(1:nrow(marriage)),breaks=k,labels=F) 
diags<-NULL

for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  
  fit<-glm(income.dollars~., data = train)
 yhat<-predict(fit, newdata = test)
  diags<-mean((test$income.dollars-yhat)^2)
}

mean(diags)

#cross validating from lasso
set.seed(1234)
k=5 
data<-marriage[sample(nrow(marriage)),] 
folds<-cut(seq(1:nrow(marriage)),breaks=k,labels=F) 

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  
  fit<-glm(income.dollars~married.percent+nevermarried.percent+age.group+gender+highincome, data = train)
 yhat<-predict(fit, newdata = test)
  diags<-mean((test$income.dollars-yhat)^2)
}

mean(diags)
``` 
###The lasso predicted which variables are most predictive of high income. The percent separated is a predictor of the percent of the population that are married. A k fold test was run to cross-validate the lasso model. The MSE of the full model was 4.41163e-23, and the MSE of prediction is 10800. When using the factors pulled from the lasso, the CV MSE is the same. 